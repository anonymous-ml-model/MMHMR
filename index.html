<!-- from https://getbootstrap.com/docs/5.3/examples/album/ -->
<!doctype html>
<html lang="en" data-bs-theme="auto">
  <head>
    <!-- <script src="/docs/5.3/assets/js/color-modes.js"></script> -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Hugo 0.118.2">
    <title>MMHMR: Generative hand Mesh Recovery</title>
    <!-- <link rel="canonical" href="https://getbootstrap.com/docs/5.3/examples/album/"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3"><link href="/docs/5.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous"><link rel="apple-touch-icon" href="/docs/5.3/assets/img/favicons/apple-touch-icon.png" sizes="180x180"><link rel="icon" href="/docs/5.3/assets/img/favicons/favicon-32x32.png" sizes="32x32" type="image/png"><link rel="icon" href="/docs/5.3/assets/img/favicons/favicon-16x16.png" sizes="16x16" type="image/png"><link rel="manifest" href="/docs/5.3/assets/img/favicons/manifest.json"><link rel="mask-icon" href="/docs/5.3/assets/img/favicons/safari-pinned-tab.svg" color="#712cf9"><link rel="icon" href="/docs/5.3/assets/img/favicons/favicon.ico"> -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
    <meta name="theme-color" content="#712cf9">
    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }

      .b-example-divider {
        width: 100%;
        height: 3rem;
        background-color: rgba(0, 0, 0, .1);
        border: solid rgba(0, 0, 0, .15);
        border-width: 1px 0;
        box-shadow: inset 0 .5em 1.5em rgba(0, 0, 0, .1), inset 0 .125em .5em rgba(0, 0, 0, .15);
      }

      .b-example-vr {
        flex-shrink: 0;
        width: 1.5rem;
        height: 100vh;
      }

      .bi {
        vertical-align: -.125em;
        fill: currentColor;
      }

      .nav-scroller {
        position: relative;
        z-index: 2;
        height: 2.75rem;
        overflow-y: hidden;
      }

      .nav-scroller .nav {
        display: flex;
        flex-wrap: nowrap;
        padding-bottom: 1rem;
        margin-top: -1px;
        overflow-x: auto;
        text-align: center;
        white-space: nowrap;
        -webkit-overflow-scrolling: touch;
      }

      .btn-bd-primary {
        --bd-violet-bg: #712cf9;
        --bd-violet-rgb: 112.520718, 44.062154, 249.437846;
        --bs-btn-font-weight: 600;
        --bs-btn-color: var(--bs-white);
        --bs-btn-bg: var(--bd-violet-bg);
        --bs-btn-border-color: var(--bd-violet-bg);
        --bs-btn-hover-color: var(--bs-white);
        --bs-btn-hover-bg: #6528e0;
        --bs-btn-hover-border-color: #6528e0;
        --bs-btn-focus-shadow-rgb: var(--bd-violet-rgb);
        --bs-btn-active-color: var(--bs-btn-hover-color);
        --bs-btn-active-bg: #5a23c8;
        --bs-btn-active-border-color: #5a23c8;
      }

      .bd-mode-toggle {
        z-index: 1500;
      }

      .bd-mode-toggle .dropdown-menu .active .bi {
        display: block !important;
      }

      .masthead {
        height: 100vh;
        min-height: 500px;
        background-image: url('./website/head.jpg');
        background-size: cover;
        background-position: center;
        background-repeat: no-repeat;
      }

      .btn span.icon {
        background: url(assets/arxiv-logomark-small.svg) no-repeat;
        float: left;
        width: 10px;
        height: 40px;
      }

      .arxiv-icon {
        background-image: url(assets/arxiv-logomark-small.svg);
        background-size: cover;
        display: inline-block;
        background-position: center center;
        height: 24px;
        width: 24px;
      }

      .mmm-card {
        border-color: #660000;
        border-style: solid;
        border-width: 2px;
      }

      .custom-video-size {
        width: 100%;
        /* Sets the width of the video to 100% of its container */
        width: 100%;
        /* Sets the width of the video to 100% of its container */
      }

      .card-body {
        min-height: 120px;
        /* Set a minimum height */
        display: flex;
        flex-direction: column;
        justify-content: space-between;
      }

      body {
        background-color: #ffffff;
        /* Or any other color code */
      }

      video {
        height: auto
      }
    </style>
  </head>
  <body>
    <main>
      <section class="py-5 text-center container">
        <div class="row py-lg-5">
          <div class="col-lg-12 col-md-12 mx-auto">
            <h1>MMHMR: Generative Masked Modeling for Hand Mesh Recovery</h1>
            <h5>anonymous</h5>
            <p></p>
            <!-- <a type="button" class="btn btn-outline-secondary" href="https://arxiv.org/abs/2403.19435"><svg width="16" height="20"><image xlink:href="assets/arxiv-logomark-small.svg" src="assets/arxiv-logomark-small.svg"
			width="16" height="16" /></svg>
		arXiv
	    </a> -->
            <!-- <a type="button" class="btn btn-outline-secondary" href="https://github.com/exitudio/BAMM/"><svg
											xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                            class="bi bi-github" viewBox="0 0 16 16"><path
                                d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8"></path></svg>
                        Code
                    </a> -->
          </div>
          <!-- <div class="d-flex justify-content-center" style="padding-top: 50px; width: 100%;"><img src="./assets/images/BAMM-Landing.png" class="img-fluid" alt="Responsive image" style="width: 100%;"></div><div class="caption" style="margin-top: 10px;"><strong>(a) Motion Length Prediction:</strong> Text-to-motion models often require specific input lengths, making them sensitive to motion generation. In contrast, BAMM automatically predicts the end of the motion, thus avoiding reliance on inaccurate motion length estimations. <strong>(b) High-quality Text-to-Motion:</strong> BAMM generates natural hand movements precisely aligned with detailed textual descriptions. <strong>(c) Motion Editing:</strong> BAMM is capable of multiple editing tasks, such as inpainting (as demonstrated), outpainting, prefix prediction, suffix completion . <strong>(d) Long Sequence Generation:</strong> BAMM can generate arbitrarily long motion sequence synthesis. <p> In this figure, <strong style="color: #255F83;">blue frames</strong> indicate individual motion segments derived from text descriptions or predefined conditions (for editing tasks). <strong style="color: #660000;">Red frames</strong> highlight the sections that have been modified, while <strong style="color: rgb(255, 204, 0);">yellow frames</strong> depict the intermediate transitions between these prompted segments. </p></div> -->
        </div>
      </section>
      <!-- <div class="album py-5 bg-body-tertiary"><div class="container"><h3>Abstract</h3><div>Generating hand motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D hand motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on handML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures. </div></div></div> -->
      </div>
      <section class="py-5 text-center container">
        <div class="row py-lg-5">
          <div class="col-lg-8 col-md-8 mx-auto">
            <h1>MMHMR DEMO</h1>
          </div>
        </div>
      </section>
      <div class="album py-5 bg-body-tertiary">
        <div class="container">
          <div class="row">
            <div class="col">
              <!-- <h3>Length Diversity</h3> -->
              <!-- <h6>Given a text prompt, BAMM generates diverse motion sequences along with diverse sequence lengths. MoMask and MMM face challenges to generate motion with the sequence length that is different from the ground-truth length . </h6>
              <strong>Text prompt : <span style='color:#255F83;'>“the person crouches and walks forward.”</span> (ground truth length: 196 frames). </strong>
              <ul>
                <li>
                  <b>BAMM:</b> Realistic, diverse crouch-walk sequences; superior in motion fidelity.
                </li>
              </ul> -->
            </div>
          </div>
          <!-- <h4 class="mt-4">196 Frame Sequences (Ground Truth)</h4> -->
          <!-- Section for 184 frames -->
          <!-- Section for 156 frames -->
          <!-- <div class="col">
            <div class="card shadow-sm">
              <video id="dance" autoplay controls muted loop playsinline height="100%">
                <source src="./assets/demo/demo.mp4" type="video/mp4">
              </video>
              <div class="card-body">
                <!-- <h6 class="card-text">MMM</h6> -->
                <p>The MMHMR demo highlights our method for achieving accurate 3D hand mesh reconstructions from monocular images. Key to our approach is <strong>Uncertainty-Guided Sampling</strong> (UGS), which iteratively samples high-confidence pose tokens based on their probabilistic distributions, ensuring precise and reliable 3D reconstructions. Here, we show only the initial pose estimates from UGS for 5 iterations.</p>          </div>
                <!-- <p>lacks any crouching motion in the sequence.</p> -->
              </div>
            </div>
          </div> -->
        </div>
      </div>


      <div class="album py-5">
        <div class="container">
          <h3>MMHMR <i>Training Phase</i>
          </h3>
          <p style="font-size: 1.2em;">MMHMR consists of two key components: (1) a <strong>Pose Tokenizer</strong> that encodes 3D hand poses into a sequence of discrete tokens within a latent space, and (2) an Image-Conditioned Masked Transformer that models the probabilistic distributions of these tokens, conditioned on the input image and a partially masked token sequence.</p>
          <img src="./assets/arch/MMHMR_landing.png" class="img-fluid" alt="MMHMR Overview Image">
        </div>
      </div>


      <div class="album py-5">
        <div class="container">
          <h3>MMHMR <i>Training Phase</i>
          </h3>
          <p style="font-size: 1.2em;">MMHMR consists of two key components: (1) a <strong>Pose Tokenizer</strong> that encodes 3D hand poses into a sequence of discrete tokens within a latent space, and (2) an Image-Conditioned Masked Transformer that models the probabilistic distributions of these tokens, conditioned on the input image and a partially masked token sequence.</p>
          <img src="./assets/arch/MMHMR_overview.png" class="img-fluid" alt="MMHMR Overview Image">
        </div>
      </div>


      
      </div>
      </div>
      <div class="album py-5">
        <div class="container">
          <h3>Our <i>Inference Strategy</i>
          </h3>
          <p st limitations of these SOTA approaches particularly in dealing with unusual poses or ambiguous situations, are evident in the errors marked by red circles. Our approach, MMHMR, addresses these challenges by explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process, leading to more accurate and robust 3D pose reconstructions in complex scenarios.</p></p>yle="font-size: 1.2em;">Our inference strategy comprises two key stages: (1) <strong>Uncertainty-Guided Sampling</strong>, which iteratively samples high-confidence pose tokens based on their probabilistic distributions, and (2) <strong>2D Pose-Guided Refinement</strong>, which fine-tunes the sampled pose tokens to further minimize 3D reconstruction uncertainty by ensuring consistency between the 3D body mesh and 2D pose estimates. </p>
          <img src="./assets/arch/MMHMR_infer.png" class="img-fluid" alt="MMHMR Inference Image">
        </div>
      </div>
      <div class="album py-5">
        <div class="container">
          <h3>State-of-the-Art Comparison</h3>
          <p style="font-size: 1.2em;">State-of-the-art (SOTA) methods, utilize vision transformers to recover 3D hand meshes from single images. However, the
          <img src="./assets/arch/mmhmr_sota_comp.png" class="img-fluid" alt="State-of-the-Art Comparison Image">
        </div>
      </div>
      <div class="album py-5">
        <div class="container">
          <h3>Qualitative Results on Challenging Poses</h3>
          <p style="font-size: 1.2em;">Qualitative results of our approach on challenging poses from the LSP dataset..</p>
          <img src="./assets/arch/MMHMR_chall_poses_3d.png" class="img-fluid" alt="Qualitative Results on Challenging Poses Image">
        </div>
      </div>
      </div>

      <div class="album py-5">
        <div class="container">
          <h3>More Results on Challenging Pose</h3>
          <p style="font-size: 1.2em;">This figure showcases  qualitative results of our MMHMR model on challenging  poses. These results demonstrate the model's ability to accurately reconstruct complex 3D poses even in scenarios  further highlighting the robustness and effectiveness of our approach.</p>
          <img src="./assets/challenging_poses/MMHMR_front_pose.png" class="img-fluid" alt="2D Pose-Guided Refinement Image">
        </div>
      </div>




    </main>
    <footer class="text-body-secondary py-5">
      <div class="container">
        <p class="float-end mb-1">
          <a href="#">Back to top</a>
        </p>
        <!-- <p class="mb-1">Album example is &copy; Bootstrap, but please download and customize it for yourself!</p><p class="mb-0">New to Bootstrap? <a href="/">Visit the homepage</a> or read our <a
                    href="/docs/5.3/getting-started/introduction/">getting started guide</a>.</p> -->
      </div>
    </footer>
    <!-- <script src="/docs/5.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
        crossorigin="anonymous"></script> -->
  </body>
</html>